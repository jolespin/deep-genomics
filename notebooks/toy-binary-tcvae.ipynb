{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b79aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST\n",
    "print(\"Loading MNIST...\")\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='auto')\n",
    "X = (X > 0).astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "# Split into train/test (MNIST default split)\n",
    "X_train_full, X_test = X[:60000], X[60000:]\n",
    "y_train_full, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# Split train into train/validation (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, \n",
    "    y_train_full, \n",
    "    test_size=0.2,  # 20% for validation\n",
    "    random_state=42,\n",
    "    stratify=y_train_full  # Maintain class distribution\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train),\n",
    "    torch.from_numpy(y_train),\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_val),\n",
    "    torch.from_numpy(y_val),\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_test),\n",
    "    torch.from_numpy(y_test),\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "num_workers = cpu_count() - 1\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=False,  # Don't shuffle validation\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "import lightning as L\n",
    "from deep_genomics.models.vae import BinaryBetaTCVAE\n",
    "from deep_genomics.utils import set_seed\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "device = \"mps\"\n",
    "beta_grid = [1.0, 4.0, 6.0, 10.0]\n",
    "limit_train_batches=None\n",
    "max_epochs=50\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',           # What metric to watch\n",
    "    min_delta=0.001,              # Minimum change to qualify as improvement\n",
    "    patience=10,                  # Number of epochs with no improvement\n",
    "    verbose=True,                 # Print messages\n",
    "    mode='min',                   # Minimize the metric\n",
    ")\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"lightning_logs\", \n",
    "    name=\"mnist-betatcvae_betasweep\",\n",
    ")\n",
    "\n",
    "for beta in beta_grid:\n",
    "    set_seed(42)\n",
    "    input_dim = train_dataset[0][0].shape[0]\n",
    "    hidden_dims = [512,256,128]\n",
    "    latent_dim = 24\n",
    "    model = BinaryBetaTCVAE(\n",
    "        input_dim=input_dim, \n",
    "        hidden_dims=hidden_dims, \n",
    "        latent_dim=latent_dim,\n",
    "        beta=beta,\n",
    "        alpha=1.0,\n",
    "        gamma=1.0,\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        logger=logger, \n",
    "        limit_train_batches=limit_train_batches, \n",
    "        max_epochs=max_epochs, \n",
    "        accelerator=device,\n",
    "        callbacks=[early_stop_callback],\n",
    "    )\n",
    "    trainer.fit( \n",
    "        model=model, \n",
    "        train_dataloaders=train_loader, \n",
    "        val_dataloaders=val_loader,\n",
    "    )\n",
    "\n",
    "    # Visualize\n",
    "    x = X_test[0]\n",
    "    x_recon = model.reconstruct(x) #> 0.5\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(5,5))\n",
    "\n",
    "    axes[0].imshow(x.reshape(28,28), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(x_recon.reshape(28,28), cmap='gray')\n",
    "    axes[1].set_title('Reconstruction')\n",
    "    axes[1].axis('off')\n",
    "    fig.suptitle(model.experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eafb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd535b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"lightning_logs/vae_testing/pretrained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model2 = BinaryBetaVAE.from_pretrained(\"lightning_logs/vae_testing/pretrained_model\", map_location=\"mps\")\n",
    "# x = X_test[0]\n",
    "# x_recon = model2.reconstruct(x) #> 0.5\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(5,5))\n",
    "\n",
    "# axes[0].imshow(x.reshape(28,28), cmap='gray')\n",
    "# axes[0].set_title('Original')\n",
    "# axes[0].axis('off')\n",
    "\n",
    "# axes[1].imshow(x_recon.reshape(28,28), cmap='gray')\n",
    "# axes[1].set_title('Reconstruction')\n",
    "# axes[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f62553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_token = \"\"\n",
    "# model.push_to_hub(\"jolespin/binary-vae-mnist\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_latent = model.transform(torch.from_numpy(X_train).to(device)).cpu().numpy()\n",
    "\n",
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Convert tensor to numpy array\n",
    "\n",
    "# # Run UMAP to reduce to 2D\n",
    "# reducer = umap.UMAP(n_components=2)\n",
    "# X_umap = reducer.fit_transform(X_latent)\n",
    "\n",
    "# # Ensure labels are integers for coloring\n",
    "# labels = y_train.astype(int)\n",
    "\n",
    "# # Plot with digit colors\n",
    "# plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, cmap='tab10', s=5)\n",
    "# plt.title('UMAP projection (MNIST digits colored)')\n",
    "# plt.xlabel('UMAP 1')\n",
    "# plt.ylabel('UMAP 2')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8730d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.distributions import (\n",
    "#     Normal, \n",
    "#     Bernoulli, \n",
    "#     kl_divergence,\n",
    "# )\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.datasets import fetch_openml\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import cpu_count\n",
    "# from pathlib import Path\n",
    "# import json\n",
    "# from abc import (\n",
    "#     ABC,\n",
    "#     abstractmethod,\n",
    "# )\n",
    "# from loguru import logger\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.distributions import (\n",
    "#     Normal, \n",
    "#     Bernoulli,\n",
    "#     kl_divergence,\n",
    "# )\n",
    "# import lightning as L\n",
    "# from huggingface_hub import (\n",
    "#     HfApi, \n",
    "#     hf_hub_download,\n",
    "# )\n",
    "\n",
    "\n",
    "# from deep_genomics.utils import (\n",
    "#     get_activation_fn,\n",
    "#     ACTIVATION_MAP,\n",
    "#     set_seed,\n",
    "# )\n",
    "# from deep_genomics.losses import (\n",
    "#     beta_vae_loss, \n",
    "#     beta_tcvae_loss,\n",
    "# )\n",
    "# from deep_genomics.metrics import (\n",
    "#     binary_confusion_matrix,\n",
    "# )\n",
    "\n",
    "\n",
    "# class VariationalEncoder(nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             input_dim: int,\n",
    "#             hidden_dims: list,\n",
    "#             latent_dim: int,\n",
    "#             activation_fn = nn.ReLU,\n",
    "#         ):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dims = list(hidden_dims)\n",
    "#         self.latent_dim = latent_dim\n",
    "#         if isinstance(activation_fn, str):\n",
    "#             activation_fn = ACTIVATION_MAP[activation_fn]\n",
    "#         self.activation_fn = activation_fn\n",
    "        \n",
    "#         # Build encoder with progressive compression\n",
    "#         encoder_layers = []\n",
    "#         prev_dim = input_dim\n",
    "        \n",
    "#         for hidden_dim in hidden_dims:\n",
    "#             encoder_layers.extend([\n",
    "#                 nn.Linear(prev_dim, hidden_dim),\n",
    "#                 activation_fn(),\n",
    "#             ])\n",
    "#             prev_dim = hidden_dim\n",
    "        \n",
    "#         self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "#         # Encoder heads\n",
    "#         last_hidden_dim = hidden_dims[-1]\n",
    "#         self.fc_mu = nn.Linear(last_hidden_dim, latent_dim)\n",
    "#         self.fc_logvar = nn.Linear(last_hidden_dim, latent_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = self.encoder(x)\n",
    "#         mu = self.fc_mu(h)\n",
    "#         logvar = self.fc_logvar(h)\n",
    "#         return mu, logvar\n",
    "        \n",
    "# class VariationalDecoder(nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             input_dim: int,\n",
    "#             hidden_dims: list,\n",
    "#             latent_dim: int,\n",
    "#             activation_fn = nn.ReLU,\n",
    "#         ):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dims = list(hidden_dims)\n",
    "#         self.latent_dim = latent_dim\n",
    "#         if isinstance(activation_fn, str):\n",
    "#             activation_fn = ACTIVATION_MAP[activation_fn]\n",
    "#         self.activation_fn = activation_fn\n",
    "\n",
    "\n",
    "#         # Build decoder - mirror encoder\n",
    "#         decoder_layers = []\n",
    "#         prev_dim = latent_dim\n",
    "        \n",
    "#         for hidden_dim in reversed(hidden_dims):\n",
    "#             decoder_layers.extend([\n",
    "#                 nn.Linear(prev_dim, hidden_dim),\n",
    "#                 activation_fn()\n",
    "#             ])\n",
    "#             prev_dim = hidden_dim\n",
    "        \n",
    "#         # Final layer outputs logits (not probabilities)\n",
    "#         decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "#         self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         logits = self.decoder(z)\n",
    "#         return logits\n",
    "    \n",
    "# class BaseVAE(L.LightningModule, ABC):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             encoder: nn.Module,\n",
    "#             decoder: nn.Module,\n",
    "#             learning_rate:float,\n",
    "#             ) -> None:\n",
    "#         super().__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.learning_rate = learning_rate\n",
    "#         # Ignore encoder/decoder save the rest\n",
    "#         self.save_hyperparameters(ignore=['encoder', 'decoder'])\n",
    "#     # Default abstract methods\n",
    "#     @abstractmethod\n",
    "#     def encode(self, x: torch.Tensor):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def decode(self, z: torch.Tensor):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"Return (p_x, q_z, z) for loss computation\"\"\"\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def sample(self, n_samples:int, device=None):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def reconstruct(self, x, batch_size=2048, device=None, return_cpu=True):\n",
    "#         pass\n",
    "#     def transform(self, x, batch_size=2048, device=None, return_cpu=True):\n",
    "#         \"\"\"\n",
    "#         Transform input to latent representation (deterministic)\n",
    "        \n",
    "#         Args:\n",
    "#             x: Tensor or array of shape [n_samples, n_features]\n",
    "#             batch_size: Process in batches of this size for memory efficiency\n",
    "#             device: Device to move tensors to (e.g., 'cpu', 'mps', 'cuda'). If None, uses model's current device\n",
    "#             return_cpu: Return output on CPU\n",
    "        \n",
    "#         Returns:\n",
    "#             Latent representations of shape [n_samples, latent_dim]\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "        \n",
    "#         if device is None:\n",
    "#             device = next(self.parameters()).device\n",
    "        \n",
    "#         # Convert to tensor if needed\n",
    "#         if not isinstance(x, torch.Tensor):\n",
    "#             x = torch.from_numpy(x).float()\n",
    "        \n",
    "#         n_samples = x.shape[0]\n",
    "#         latent_codes = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             # Process in batches for memory efficiency\n",
    "#             for i in range(0, n_samples, batch_size):\n",
    "#                 batch = x[i:i+batch_size].to(device)\n",
    "#                 mu, _ = self.encode(batch)\n",
    "#                 latent_codes.append(mu.cpu() if return_cpu else mu)\n",
    "        \n",
    "#         return torch.cat(latent_codes, dim=0)\n",
    "\n",
    "#     # Lightning Methods\n",
    "#     @abstractmethod\n",
    "#     def configure_optimizers(self):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         pass\n",
    "#     # @abstractmethod\n",
    "#     # # def test_step(self, batch, batch_idx):\n",
    "#     #     pass\n",
    "#     # @abstractmethod\n",
    "#     # # def predict_step(self, batch, batch_idx):\n",
    "#     #     pass\n",
    "\n",
    "#     # HuggingFace\n",
    "#     def save_pretrained(self, save_directory):\n",
    "#         \"\"\"\n",
    "#         Save model weights and config in HuggingFace format.\n",
    "        \n",
    "#         Args:\n",
    "#             save_directory: Path to directory where model will be saved\n",
    "#         \"\"\"\n",
    "#         save_directory = Path(save_directory)\n",
    "#         save_directory.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         # Save model weights (PyTorch state_dict)\n",
    "#         weights_path = save_directory / \"pytorch_model.bin\"\n",
    "#         torch.save(self.state_dict(), weights_path)\n",
    "        \n",
    "#         # Save hyperparameters as config\n",
    "#         config_path = save_directory / \"config.json\"\n",
    "#         with open(config_path, 'w') as f:\n",
    "#             json.dump(self.hparams, f, indent=2, default=str)\n",
    "        \n",
    "#         print(f\"Model saved to {save_directory}\")\n",
    "    \n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, pretrained_model_path, map_location=None):\n",
    "#         \"\"\"\n",
    "#         Load model from HuggingFace format or local directory.\n",
    "        \n",
    "#         Args:\n",
    "#             pretrained_model_path: Local path or HuggingFace Hub model ID\n",
    "#             map_location: Device to load model weights (e.g., 'cpu', 'cuda', 'mps')\n",
    "        \n",
    "#         Returns:\n",
    "#             Loaded model instance\n",
    "#         \"\"\"\n",
    "#         pretrained_model_path = Path(pretrained_model_path)\n",
    "        \n",
    "#         # Load config\n",
    "#         config_path = pretrained_model_path / 'config.json'\n",
    "#         if not config_path.exists():\n",
    "#             raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "        \n",
    "#         with open(config_path, 'r') as f:\n",
    "#             config = json.load(f)\n",
    "        \n",
    "#         # Create model instance (subclass must handle this)\n",
    "#         # This is why it's a classmethod - subclass implements construction logic\n",
    "#         model = cls(**config)\n",
    "        \n",
    "#         # Load weights\n",
    "#         weights_path = pretrained_model_path / 'pytorch_model.bin'\n",
    "#         if not weights_path.exists():\n",
    "#             raise FileNotFoundError(f\"Weights file not found: {weights_path}\")\n",
    "        \n",
    "#         state_dict = torch.load(weights_path, map_location=map_location)\n",
    "#         model.load_state_dict(state_dict)\n",
    "        \n",
    "#         print(f\"Model loaded from {pretrained_model_path}\")\n",
    "#         return model\n",
    "    \n",
    "#     def push_to_hub(\n",
    "#         self,\n",
    "#         repo_id: str,\n",
    "#         commit_message: str = \"Upload model\",\n",
    "#         private: bool = False,\n",
    "#         token: str = None,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Upload model to HuggingFace Hub.\n",
    "        \n",
    "#         Args:\n",
    "#             repo_id: Repository ID on HuggingFace Hub (e.g., \"username/model-name\")\n",
    "#             commit_message: Commit message for the upload\n",
    "#             private: Whether to make the repository private\n",
    "#             token: HuggingFace API token (or set HF_TOKEN environment variable)\n",
    "        \n",
    "#         Example:\n",
    "#             model.push_to_hub(\"myusername/binary-vae-mnist\")\n",
    "#         \"\"\"\n",
    "#         from huggingface_hub import HfApi\n",
    "        \n",
    "#         # Save to temporary directory\n",
    "#         import tempfile\n",
    "#         with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "#             self.save_pretrained(tmp_dir)\n",
    "            \n",
    "#             # Upload to Hub\n",
    "#             api = HfApi()\n",
    "#             api.create_repo(\n",
    "#                 repo_id=repo_id,\n",
    "#                 private=private,\n",
    "#                 exist_ok=True,\n",
    "#                 token=token,\n",
    "#             )\n",
    "            \n",
    "#             api.upload_folder(\n",
    "#                 folder_path=tmp_dir,\n",
    "#                 repo_id=repo_id,\n",
    "#                 commit_message=commit_message,\n",
    "#                 token=token,\n",
    "#             )\n",
    "        \n",
    "#         print(f\"Model uploaded to https://huggingface.co/{repo_id}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BinaryBetaVAE(BaseVAE):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             # Architecture\n",
    "#             input_dim: int,\n",
    "#             hidden_dims: list,\n",
    "#             latent_dim: int,\n",
    "#             activation_fn = \"ReLU\",\n",
    "#             # Optimizer\n",
    "#             learning_rate:float = 1e-3,\n",
    "#             # Loss\n",
    "#             beta:float = 1.0,\n",
    "#             # Sub-models\n",
    "#             encoder: nn.Module = None,\n",
    "#             decoder: nn.Module = None,\n",
    "#             ) -> None:\n",
    "#         # Get activation function\n",
    "#         if not isinstance(activation_fn, str):\n",
    "#             activation_fn = activation_fn.__name__\n",
    "#         # Build encoder/decoder if not provided\n",
    "#         if all([\n",
    "#             encoder is None,\n",
    "#             decoder is None,\n",
    "#             ]):\n",
    "#             # Neither provided - build both\n",
    "#             if any([\n",
    "#                 input_dim is None, \n",
    "#                 hidden_dims is None, \n",
    "#                 latent_dim is None,\n",
    "#                 ]):\n",
    "#                 raise ValueError(\n",
    "#                     \"Must provide either (encoder, decoder) or \"\n",
    "#                     \"(input_dim, hidden_dims, latent_dim)\"\n",
    "#                 )\n",
    "#             encoder = VariationalEncoder(input_dim, hidden_dims, latent_dim, activation_fn)\n",
    "#             decoder = VariationalDecoder(input_dim, hidden_dims, latent_dim, activation_fn)\n",
    "            \n",
    "#         elif encoder is None or decoder is None:\n",
    "#             # Only one provided - error\n",
    "#             raise ValueError(\"Must provide both encoder and decoder, or neither\")\n",
    "            \n",
    "#         else:\n",
    "#             # Both provided - validate they match\n",
    "#             if encoder.input_dim != decoder.input_dim:\n",
    "#                 raise ValueError(\"Encoder and decoder input_dim must match\")\n",
    "#             if list(encoder.hidden_dims) != list(decoder.hidden_dims):\n",
    "#                 raise ValueError(\"Encoder and decoder hidden_dims must match\")\n",
    "#             if encoder.latent_dim != decoder.latent_dim:\n",
    "#                 raise ValueError(\"Encoder and decoder latent_dim must match\")\n",
    "            \n",
    "#             # Infer architecture from encoder\n",
    "#             input_dim = encoder.input_dim\n",
    "#             hidden_dims = encoder.hidden_dims\n",
    "#             latent_dim = encoder.latent_dim\n",
    "#         super().__init__(encoder, decoder, learning_rate)\n",
    "#         # Store archtecture metadata\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dims = list(hidden_dims)\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.activation_fn = activation_fn\n",
    "#         self.beta = beta\n",
    "#         self.save_hyperparameters(ignore=['encoder', 'decoder'])\n",
    "\n",
    "#     def encode(self, x: torch.Tensor):\n",
    "#         return self.encoder(x)\n",
    "#     def decode(self, z: torch.Tensor):\n",
    "#         return self.decoder(z)\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"Return (p_x, q_z, z) for loss computation\"\"\"\n",
    "#                 # Encode\n",
    "#         mu, logvar = self.encode(x)\n",
    "\n",
    "#         # Reparameterization\n",
    "#         std = torch.exp(0.5 * logvar)\n",
    "#         # q_z is the approximate posterior - q(z|x)\n",
    "#         q_z = Normal(mu, std)\n",
    "#         z = q_z.rsample()\n",
    "\n",
    "#         # Decode\n",
    "#         logits = self.decode(z)\n",
    "#         # p_x is the likelihood - p(x|z)\n",
    "#         p_x = Bernoulli(logits=logits)\n",
    "\n",
    "#         return p_x, q_z, z\n",
    "    \n",
    "#     def sample(self, n_samples:int, device=None):\n",
    "#         \"\"\"\n",
    "#         Generate samples from the prior p(z) = N(0, I)\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "#         if device is None:\n",
    "#             device = next(self.parameters()).device\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             # Sample from distribution\n",
    "#             z = torch.randn(n_samples, self.latent_dim, device=device)\n",
    "        \n",
    "#             # Decode to get reconstructions\n",
    "#             logits = self.decode(z)\n",
    "#             samples = torch.sigmoid(logits)\n",
    "            \n",
    "#         return samples\n",
    "#     def reconstruct(self, x, batch_size=2048, device=None, return_cpu=True):\n",
    "#         \"\"\"\n",
    "#         Reconstruct input using posterior mean (deterministic)\n",
    "\n",
    "#         Args:\n",
    "#             x: Tensor or array of shape [n_samples, n_features] or [n_features]\n",
    "#             batch_size: Process in batches of this size for memory efficiency\n",
    "#             device: Device to move tensors to (e.g., 'cpu', 'mps', 'cuda'). If None, uses model's current device\n",
    "#             return_cpu: Return output on CPU\n",
    "        \n",
    "#         Returns:\n",
    "#             Reconstruction(s) of shape [n_samples, n_features] or [n_features]\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "        \n",
    "#         if device is None:\n",
    "#             device = next(self.parameters()).device\n",
    "        \n",
    "#         # Handle single sample\n",
    "#         squeeze_output = False\n",
    "#         if isinstance(x, torch.Tensor) and x.ndim == 1:\n",
    "#             x = x.unsqueeze(0)\n",
    "#             squeeze_output = True\n",
    "#         elif isinstance(x, np.ndarray) and x.ndim == 1:\n",
    "#             x = x.reshape(1, -1)\n",
    "#             squeeze_output = True\n",
    "        \n",
    "#         # Convert to tensor if needed\n",
    "#         if not isinstance(x, torch.Tensor):\n",
    "#             x = torch.from_numpy(x).float()\n",
    "        \n",
    "#         n_samples = x.shape[0]\n",
    "#         reconstructions = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             # Process in batches for memory efficiency\n",
    "#             for i in range(0, n_samples, batch_size):\n",
    "#                 batch = x[i:i+batch_size].to(device)\n",
    "#                 mu, _ = self.encode(batch)\n",
    "#                 logits = self.decode(mu)\n",
    "#                 x_recon = torch.sigmoid(logits)\n",
    "#                 reconstructions.append(x_recon.cpu() if return_cpu else x_recon)\n",
    "        \n",
    "#         result = torch.cat(reconstructions, dim=0)\n",
    "        \n",
    "#         # Remove batch dimension if input was single sample\n",
    "#         if squeeze_output:\n",
    "#             result = result.squeeze(0)\n",
    "        \n",
    "#         return result\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             self.parameters(), \n",
    "#             lr=self.learning_rate,\n",
    "#         )\n",
    "#         return optimizer\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         # Get data\n",
    "#         x = batch[0]\n",
    "\n",
    "#         # Forward pass\n",
    "#         p_x, q_z, z = self.forward(x)\n",
    "\n",
    "#         # Compute loss\n",
    "#         losses = beta_vae_loss(\n",
    "#             x=x,\n",
    "#             p_x=p_x,\n",
    "#             q_z=q_z,\n",
    "#             beta=self.beta,\n",
    "#         )\n",
    "\n",
    "#         # Compute reconstruction metrics\n",
    "#         x_recon = torch.sigmoid(p_x.logits)\n",
    "#         confusion_matrix = binary_confusion_matrix(x_recon, x, threshold=0.5)\n",
    "\n",
    "#         # Log loss\n",
    "#         self.log(\"train_loss\", losses[\"total_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"train_recon_loss\", losses[\"recon_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"train_kl_loss\", losses[\"kl_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         # Log reconstruction metrics\n",
    "#         self.log('train_precision', confusion_matrix['precision'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_recall', confusion_matrix['recall'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_f1', confusion_matrix['f1'], on_step=False, on_epoch=True)\n",
    "\n",
    "#         # Return total loss for backpropagation\n",
    "#         return losses[\"total_loss\"]\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         # Get data\n",
    "#         x = batch[0]\n",
    "\n",
    "#         # Forward pass\n",
    "#         p_x, q_z, z = self.forward(x)\n",
    "\n",
    "#         # Compute loss\n",
    "#         losses = beta_vae_loss(\n",
    "#             x=x,\n",
    "#             p_x=p_x,\n",
    "#             q_z=q_z,\n",
    "#             beta=self.beta,\n",
    "#         )\n",
    "\n",
    "#         # Compute reconstruction metrics\n",
    "#         x_recon = torch.sigmoid(p_x.logits)\n",
    "#         confusion_matrix = binary_confusion_matrix(x_recon, x, threshold=0.5)\n",
    "\n",
    "#         # Log metrics\n",
    "#         self.log(\"val_loss\", losses[\"total_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"val_recon_loss\", losses[\"recon_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"val_kl_loss\", losses[\"kl_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         # Log reconstruction metrics\n",
    "#         self.log('train_precision', confusion_matrix['precision'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_recall', confusion_matrix['recall'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_f1', confusion_matrix['f1'], on_step=False, on_epoch=True)\n",
    "        \n",
    "#         # Return total loss for backpropagation\n",
    "#         return losses[\"total_loss\"]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
