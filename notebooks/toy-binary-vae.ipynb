{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b79aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST...\n",
      "Train samples: 48000\n",
      "Validation samples: 12000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST\n",
    "print(\"Loading MNIST...\")\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='auto')\n",
    "X = (X > 0).astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "# Split into train/test (MNIST default split)\n",
    "X_train_full, X_test = X[:60000], X[60000:]\n",
    "y_train_full, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# Split train into train/validation (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, \n",
    "    y_train_full, \n",
    "    test_size=0.2,  # 20% for validation\n",
    "    random_state=42,\n",
    "    stratify=y_train_full  # Maintain class distribution\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train),\n",
    "    torch.from_numpy(y_train),\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_val),\n",
    "    torch.from_numpy(y_val),\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_test),\n",
    "    torch.from_numpy(y_test),\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "num_workers = cpu_count() - 1\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=False,  # Don't shuffle validation\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-12 22:54:44.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdeep_genomics.utils\u001b[0m:\u001b[36mset_seed\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mRandom seed set to 42\u001b[0m\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name    | Type               | Params | Mode  | FLOPs\n",
      "---------------------------------------------------------------\n",
      "0 | encoder | VariationalEncoder | 572 K  | train | 0    \n",
      "1 | decoder | VariationalDecoder | 570 K  | train | 0    \n",
      "---------------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.569     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdd2b7f2dfc4f49b040b8efb0eaf347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f26b232c6647b597f6f88e05352b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab17bdcb47164201a0e6b33f16da34cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b30afe90794dd4aa67725d753f715f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6f198ac5b84646a2ea82f0980ccf11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8355a94fd25249e2a0f8c8e74c78e9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67da8ab312e46ef95e3c12ee5194104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7048a41cfac4d42950c56e35bd2c61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd667d2b3584280b3a3a568e9588a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc0ba678ee24f10b49286bf68203357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126b209195ef45759616700812edf9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c98d42a1314312b53a3f22ac17e14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "import lightning as L\n",
    "from deep_genomics.models.vae import BinaryBetaVAE\n",
    "from deep_genomics.utils import set_seed\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "# from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "\n",
    "\n",
    "device = \"mps\"\n",
    "set_seed(42)\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"vae_testing\")\n",
    "\n",
    "input_dim = train_dataset[0][0].shape[0]\n",
    "hidden_dims = [512,256,128]\n",
    "latent_dim = 24\n",
    "model = BinaryBetaVAE(\n",
    "    input_dim=input_dim, \n",
    "    hidden_dims=hidden_dims, \n",
    "    latent_dim=latent_dim,\n",
    ")\n",
    "limit_train_batches=None\n",
    "max_epochs=10\n",
    "trainer = L.Trainer(\n",
    "    logger=logger, \n",
    "    limit_train_batches=limit_train_batches, \n",
    "    max_epochs=max_epochs, \n",
    "    accelerator=device,\n",
    ")\n",
    "trainer.fit( model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4eafb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(27.5), np.float64(27.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = X_test[0]\n",
    "x_recon = model.reconstruct(x) #> 0.5\n",
    "fig, axes = plt.subplots(1, 2, figsize=(5,5))\n",
    "\n",
    "axes[0].imshow(x.reshape(28,28), cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(x_recon.reshape(28,28), cmap='gray')\n",
    "axes[1].set_title('Reconstruction')\n",
    "axes[1].axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd535b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to lightning_logs/vae_testing/pretrained_model\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"lightning_logs/vae_testing/pretrained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from lightning_logs/vae_testing/pretrained_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model2 = BinaryBetaVAE.from_pretrained(\"lightning_logs/vae_testing/pretrained_model\", map_location=\"mps\")\n",
    "x = X_test[0]\n",
    "x_recon = model2.reconstruct(x) #> 0.5\n",
    "fig, axes = plt.subplots(1, 2, figsize=(5,5))\n",
    "\n",
    "axes[0].imshow(x.reshape(28,28), cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(x_recon.reshape(28,28), cmap='gray')\n",
    "axes[1].set_title('Reconstruction')\n",
    "axes[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f62553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /var/folders/kz/sjn7w_851dld8bdf3jf6hbw80000gn/T/tmp2962v7k3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42235ee8c4f74e2bb95fd589de9907d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2eafab94984286b196b7abd03820bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to https://huggingface.co/jolespin/binary-vae-mnist\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"\"\n",
    "model.push_to_hub(\"jolespin/binary-vae-mnist\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89c58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_latent = model.transform(torch.from_numpy(X_train).to(device)).cpu().numpy()\n",
    "\n",
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Convert tensor to numpy array\n",
    "\n",
    "# # Run UMAP to reduce to 2D\n",
    "# reducer = umap.UMAP(n_components=2)\n",
    "# X_umap = reducer.fit_transform(X_latent)\n",
    "\n",
    "# # Ensure labels are integers for coloring\n",
    "# labels = y_train.astype(int)\n",
    "\n",
    "# # Plot with digit colors\n",
    "# plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, cmap='tab10', s=5)\n",
    "# plt.title('UMAP projection (MNIST digits colored)')\n",
    "# plt.xlabel('UMAP 1')\n",
    "# plt.ylabel('UMAP 2')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8730d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.distributions import (\n",
    "#     Normal, \n",
    "#     Bernoulli, \n",
    "#     kl_divergence,\n",
    "# )\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.datasets import fetch_openml\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import cpu_count\n",
    "# from pathlib import Path\n",
    "# import json\n",
    "# from abc import (\n",
    "#     ABC,\n",
    "#     abstractmethod,\n",
    "# )\n",
    "# from loguru import logger\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.distributions import (\n",
    "#     Normal, \n",
    "#     Bernoulli,\n",
    "#     kl_divergence,\n",
    "# )\n",
    "# import lightning as L\n",
    "# from huggingface_hub import (\n",
    "#     HfApi, \n",
    "#     hf_hub_download,\n",
    "# )\n",
    "\n",
    "\n",
    "# from deep_genomics.utils import (\n",
    "#     get_activation_fn,\n",
    "#     ACTIVATION_MAP,\n",
    "#     set_seed,\n",
    "# )\n",
    "# from deep_genomics.losses import (\n",
    "#     beta_vae_loss, \n",
    "#     beta_tcvae_loss,\n",
    "# )\n",
    "# from deep_genomics.metrics import (\n",
    "#     binary_confusion_matrix,\n",
    "# )\n",
    "\n",
    "\n",
    "# class VariationalEncoder(nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             input_dim: int,\n",
    "#             hidden_dims: list,\n",
    "#             latent_dim: int,\n",
    "#             activation_fn = nn.ReLU,\n",
    "#         ):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dims = list(hidden_dims)\n",
    "#         self.latent_dim = latent_dim\n",
    "#         if isinstance(activation_fn, str):\n",
    "#             activation_fn = ACTIVATION_MAP[activation_fn]\n",
    "#         self.activation_fn = activation_fn\n",
    "        \n",
    "#         # Build encoder with progressive compression\n",
    "#         encoder_layers = []\n",
    "#         prev_dim = input_dim\n",
    "        \n",
    "#         for hidden_dim in hidden_dims:\n",
    "#             encoder_layers.extend([\n",
    "#                 nn.Linear(prev_dim, hidden_dim),\n",
    "#                 activation_fn(),\n",
    "#             ])\n",
    "#             prev_dim = hidden_dim\n",
    "        \n",
    "#         self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "#         # Encoder heads\n",
    "#         last_hidden_dim = hidden_dims[-1]\n",
    "#         self.fc_mu = nn.Linear(last_hidden_dim, latent_dim)\n",
    "#         self.fc_logvar = nn.Linear(last_hidden_dim, latent_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = self.encoder(x)\n",
    "#         mu = self.fc_mu(h)\n",
    "#         logvar = self.fc_logvar(h)\n",
    "#         return mu, logvar\n",
    "        \n",
    "# class VariationalDecoder(nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             input_dim: int,\n",
    "#             hidden_dims: list,\n",
    "#             latent_dim: int,\n",
    "#             activation_fn = nn.ReLU,\n",
    "#         ):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dims = list(hidden_dims)\n",
    "#         self.latent_dim = latent_dim\n",
    "#         if isinstance(activation_fn, str):\n",
    "#             activation_fn = ACTIVATION_MAP[activation_fn]\n",
    "#         self.activation_fn = activation_fn\n",
    "\n",
    "\n",
    "#         # Build decoder - mirror encoder\n",
    "#         decoder_layers = []\n",
    "#         prev_dim = latent_dim\n",
    "        \n",
    "#         for hidden_dim in reversed(hidden_dims):\n",
    "#             decoder_layers.extend([\n",
    "#                 nn.Linear(prev_dim, hidden_dim),\n",
    "#                 activation_fn()\n",
    "#             ])\n",
    "#             prev_dim = hidden_dim\n",
    "        \n",
    "#         # Final layer outputs logits (not probabilities)\n",
    "#         decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "#         self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         logits = self.decoder(z)\n",
    "#         return logits\n",
    "    \n",
    "# class BaseVAE(L.LightningModule, ABC):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             encoder: nn.Module,\n",
    "#             decoder: nn.Module,\n",
    "#             learning_rate:float,\n",
    "#             ) -> None:\n",
    "#         super().__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.learning_rate = learning_rate\n",
    "#         # Ignore encoder/decoder save the rest\n",
    "#         self.save_hyperparameters(ignore=['encoder', 'decoder'])\n",
    "#     # Default abstract methods\n",
    "#     @abstractmethod\n",
    "#     def encode(self, x: torch.Tensor):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def decode(self, z: torch.Tensor):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"Return (p_x, q_z, z) for loss computation\"\"\"\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def sample(self, n_samples:int, device=None):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def reconstruct(self, x, batch_size=2048, device=None, return_cpu=True):\n",
    "#         pass\n",
    "#     def transform(self, x, batch_size=2048, device=None, return_cpu=True):\n",
    "#         \"\"\"\n",
    "#         Transform input to latent representation (deterministic)\n",
    "        \n",
    "#         Args:\n",
    "#             x: Tensor or array of shape [n_samples, n_features]\n",
    "#             batch_size: Process in batches of this size for memory efficiency\n",
    "#             device: Device to move tensors to (e.g., 'cpu', 'mps', 'cuda'). If None, uses model's current device\n",
    "#             return_cpu: Return output on CPU\n",
    "        \n",
    "#         Returns:\n",
    "#             Latent representations of shape [n_samples, latent_dim]\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "        \n",
    "#         if device is None:\n",
    "#             device = next(self.parameters()).device\n",
    "        \n",
    "#         # Convert to tensor if needed\n",
    "#         if not isinstance(x, torch.Tensor):\n",
    "#             x = torch.from_numpy(x).float()\n",
    "        \n",
    "#         n_samples = x.shape[0]\n",
    "#         latent_codes = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             # Process in batches for memory efficiency\n",
    "#             for i in range(0, n_samples, batch_size):\n",
    "#                 batch = x[i:i+batch_size].to(device)\n",
    "#                 mu, _ = self.encode(batch)\n",
    "#                 latent_codes.append(mu.cpu() if return_cpu else mu)\n",
    "        \n",
    "#         return torch.cat(latent_codes, dim=0)\n",
    "\n",
    "#     # Lightning Methods\n",
    "#     @abstractmethod\n",
    "#     def configure_optimizers(self):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         pass\n",
    "#     # @abstractmethod\n",
    "#     # # def test_step(self, batch, batch_idx):\n",
    "#     #     pass\n",
    "#     # @abstractmethod\n",
    "#     # # def predict_step(self, batch, batch_idx):\n",
    "#     #     pass\n",
    "\n",
    "#     # HuggingFace\n",
    "#     def save_pretrained(self, save_directory):\n",
    "#         \"\"\"\n",
    "#         Save model weights and config in HuggingFace format.\n",
    "        \n",
    "#         Args:\n",
    "#             save_directory: Path to directory where model will be saved\n",
    "#         \"\"\"\n",
    "#         save_directory = Path(save_directory)\n",
    "#         save_directory.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         # Save model weights (PyTorch state_dict)\n",
    "#         weights_path = save_directory / \"pytorch_model.bin\"\n",
    "#         torch.save(self.state_dict(), weights_path)\n",
    "        \n",
    "#         # Save hyperparameters as config\n",
    "#         config_path = save_directory / \"config.json\"\n",
    "#         with open(config_path, 'w') as f:\n",
    "#             json.dump(self.hparams, f, indent=2, default=str)\n",
    "        \n",
    "#         print(f\"Model saved to {save_directory}\")\n",
    "    \n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, pretrained_model_path, map_location=None):\n",
    "#         \"\"\"\n",
    "#         Load model from HuggingFace format or local directory.\n",
    "        \n",
    "#         Args:\n",
    "#             pretrained_model_path: Local path or HuggingFace Hub model ID\n",
    "#             map_location: Device to load model weights (e.g., 'cpu', 'cuda', 'mps')\n",
    "        \n",
    "#         Returns:\n",
    "#             Loaded model instance\n",
    "#         \"\"\"\n",
    "#         pretrained_model_path = Path(pretrained_model_path)\n",
    "        \n",
    "#         # Load config\n",
    "#         config_path = pretrained_model_path / 'config.json'\n",
    "#         if not config_path.exists():\n",
    "#             raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "        \n",
    "#         with open(config_path, 'r') as f:\n",
    "#             config = json.load(f)\n",
    "        \n",
    "#         # Create model instance (subclass must handle this)\n",
    "#         # This is why it's a classmethod - subclass implements construction logic\n",
    "#         model = cls(**config)\n",
    "        \n",
    "#         # Load weights\n",
    "#         weights_path = pretrained_model_path / 'pytorch_model.bin'\n",
    "#         if not weights_path.exists():\n",
    "#             raise FileNotFoundError(f\"Weights file not found: {weights_path}\")\n",
    "        \n",
    "#         state_dict = torch.load(weights_path, map_location=map_location)\n",
    "#         model.load_state_dict(state_dict)\n",
    "        \n",
    "#         print(f\"Model loaded from {pretrained_model_path}\")\n",
    "#         return model\n",
    "    \n",
    "#     def push_to_hub(\n",
    "#         self,\n",
    "#         repo_id: str,\n",
    "#         commit_message: str = \"Upload model\",\n",
    "#         private: bool = False,\n",
    "#         token: str = None,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Upload model to HuggingFace Hub.\n",
    "        \n",
    "#         Args:\n",
    "#             repo_id: Repository ID on HuggingFace Hub (e.g., \"username/model-name\")\n",
    "#             commit_message: Commit message for the upload\n",
    "#             private: Whether to make the repository private\n",
    "#             token: HuggingFace API token (or set HF_TOKEN environment variable)\n",
    "        \n",
    "#         Example:\n",
    "#             model.push_to_hub(\"myusername/binary-vae-mnist\")\n",
    "#         \"\"\"\n",
    "#         from huggingface_hub import HfApi\n",
    "        \n",
    "#         # Save to temporary directory\n",
    "#         import tempfile\n",
    "#         with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "#             self.save_pretrained(tmp_dir)\n",
    "            \n",
    "#             # Upload to Hub\n",
    "#             api = HfApi()\n",
    "#             api.create_repo(\n",
    "#                 repo_id=repo_id,\n",
    "#                 private=private,\n",
    "#                 exist_ok=True,\n",
    "#                 token=token,\n",
    "#             )\n",
    "            \n",
    "#             api.upload_folder(\n",
    "#                 folder_path=tmp_dir,\n",
    "#                 repo_id=repo_id,\n",
    "#                 commit_message=commit_message,\n",
    "#                 token=token,\n",
    "#             )\n",
    "        \n",
    "#         print(f\"Model uploaded to https://huggingface.co/{repo_id}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9b75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BinaryBetaVAE(BaseVAE):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             # Architecture\n",
    "#             input_dim: int,\n",
    "#             hidden_dims: list,\n",
    "#             latent_dim: int,\n",
    "#             activation_fn = \"ReLU\",\n",
    "#             # Optimizer\n",
    "#             learning_rate:float = 1e-3,\n",
    "#             # Loss\n",
    "#             beta:float = 1.0,\n",
    "#             # Sub-models\n",
    "#             encoder: nn.Module = None,\n",
    "#             decoder: nn.Module = None,\n",
    "#             ) -> None:\n",
    "#         # Get activation function\n",
    "#         if not isinstance(activation_fn, str):\n",
    "#             activation_fn = activation_fn.__name__\n",
    "#         # Build encoder/decoder if not provided\n",
    "#         if all([\n",
    "#             encoder is None,\n",
    "#             decoder is None,\n",
    "#             ]):\n",
    "#             # Neither provided - build both\n",
    "#             if any([\n",
    "#                 input_dim is None, \n",
    "#                 hidden_dims is None, \n",
    "#                 latent_dim is None,\n",
    "#                 ]):\n",
    "#                 raise ValueError(\n",
    "#                     \"Must provide either (encoder, decoder) or \"\n",
    "#                     \"(input_dim, hidden_dims, latent_dim)\"\n",
    "#                 )\n",
    "#             encoder = VariationalEncoder(input_dim, hidden_dims, latent_dim, activation_fn)\n",
    "#             decoder = VariationalDecoder(input_dim, hidden_dims, latent_dim, activation_fn)\n",
    "            \n",
    "#         elif encoder is None or decoder is None:\n",
    "#             # Only one provided - error\n",
    "#             raise ValueError(\"Must provide both encoder and decoder, or neither\")\n",
    "            \n",
    "#         else:\n",
    "#             # Both provided - validate they match\n",
    "#             if encoder.input_dim != decoder.input_dim:\n",
    "#                 raise ValueError(\"Encoder and decoder input_dim must match\")\n",
    "#             if list(encoder.hidden_dims) != list(decoder.hidden_dims):\n",
    "#                 raise ValueError(\"Encoder and decoder hidden_dims must match\")\n",
    "#             if encoder.latent_dim != decoder.latent_dim:\n",
    "#                 raise ValueError(\"Encoder and decoder latent_dim must match\")\n",
    "            \n",
    "#             # Infer architecture from encoder\n",
    "#             input_dim = encoder.input_dim\n",
    "#             hidden_dims = encoder.hidden_dims\n",
    "#             latent_dim = encoder.latent_dim\n",
    "#         super().__init__(encoder, decoder, learning_rate)\n",
    "#         # Store archtecture metadata\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dims = list(hidden_dims)\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.activation_fn = activation_fn\n",
    "#         self.beta = beta\n",
    "#         self.save_hyperparameters(ignore=['encoder', 'decoder'])\n",
    "\n",
    "#     def encode(self, x: torch.Tensor):\n",
    "#         return self.encoder(x)\n",
    "#     def decode(self, z: torch.Tensor):\n",
    "#         return self.decoder(z)\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"Return (p_x, q_z, z) for loss computation\"\"\"\n",
    "#                 # Encode\n",
    "#         mu, logvar = self.encode(x)\n",
    "\n",
    "#         # Reparameterization\n",
    "#         std = torch.exp(0.5 * logvar)\n",
    "#         # q_z is the approximate posterior - q(z|x)\n",
    "#         q_z = Normal(mu, std)\n",
    "#         z = q_z.rsample()\n",
    "\n",
    "#         # Decode\n",
    "#         logits = self.decode(z)\n",
    "#         # p_x is the likelihood - p(x|z)\n",
    "#         p_x = Bernoulli(logits=logits)\n",
    "\n",
    "#         return p_x, q_z, z\n",
    "    \n",
    "#     def sample(self, n_samples:int, device=None):\n",
    "#         \"\"\"\n",
    "#         Generate samples from the prior p(z) = N(0, I)\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "#         if device is None:\n",
    "#             device = next(self.parameters()).device\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             # Sample from distribution\n",
    "#             z = torch.randn(n_samples, self.latent_dim, device=device)\n",
    "        \n",
    "#             # Decode to get reconstructions\n",
    "#             logits = self.decode(z)\n",
    "#             samples = torch.sigmoid(logits)\n",
    "            \n",
    "#         return samples\n",
    "#     def reconstruct(self, x, batch_size=2048, device=None, return_cpu=True):\n",
    "#         \"\"\"\n",
    "#         Reconstruct input using posterior mean (deterministic)\n",
    "\n",
    "#         Args:\n",
    "#             x: Tensor or array of shape [n_samples, n_features] or [n_features]\n",
    "#             batch_size: Process in batches of this size for memory efficiency\n",
    "#             device: Device to move tensors to (e.g., 'cpu', 'mps', 'cuda'). If None, uses model's current device\n",
    "#             return_cpu: Return output on CPU\n",
    "        \n",
    "#         Returns:\n",
    "#             Reconstruction(s) of shape [n_samples, n_features] or [n_features]\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "        \n",
    "#         if device is None:\n",
    "#             device = next(self.parameters()).device\n",
    "        \n",
    "#         # Handle single sample\n",
    "#         squeeze_output = False\n",
    "#         if isinstance(x, torch.Tensor) and x.ndim == 1:\n",
    "#             x = x.unsqueeze(0)\n",
    "#             squeeze_output = True\n",
    "#         elif isinstance(x, np.ndarray) and x.ndim == 1:\n",
    "#             x = x.reshape(1, -1)\n",
    "#             squeeze_output = True\n",
    "        \n",
    "#         # Convert to tensor if needed\n",
    "#         if not isinstance(x, torch.Tensor):\n",
    "#             x = torch.from_numpy(x).float()\n",
    "        \n",
    "#         n_samples = x.shape[0]\n",
    "#         reconstructions = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             # Process in batches for memory efficiency\n",
    "#             for i in range(0, n_samples, batch_size):\n",
    "#                 batch = x[i:i+batch_size].to(device)\n",
    "#                 mu, _ = self.encode(batch)\n",
    "#                 logits = self.decode(mu)\n",
    "#                 x_recon = torch.sigmoid(logits)\n",
    "#                 reconstructions.append(x_recon.cpu() if return_cpu else x_recon)\n",
    "        \n",
    "#         result = torch.cat(reconstructions, dim=0)\n",
    "        \n",
    "#         # Remove batch dimension if input was single sample\n",
    "#         if squeeze_output:\n",
    "#             result = result.squeeze(0)\n",
    "        \n",
    "#         return result\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             self.parameters(), \n",
    "#             lr=self.learning_rate,\n",
    "#         )\n",
    "#         return optimizer\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         # Get data\n",
    "#         x = batch[0]\n",
    "\n",
    "#         # Forward pass\n",
    "#         p_x, q_z, z = self.forward(x)\n",
    "\n",
    "#         # Compute loss\n",
    "#         losses = beta_vae_loss(\n",
    "#             x=x,\n",
    "#             p_x=p_x,\n",
    "#             q_z=q_z,\n",
    "#             beta=self.beta,\n",
    "#         )\n",
    "\n",
    "#         # Compute reconstruction metrics\n",
    "#         x_recon = torch.sigmoid(p_x.logits)\n",
    "#         confusion_matrix = binary_confusion_matrix(x_recon, x, threshold=0.5)\n",
    "\n",
    "#         # Log loss\n",
    "#         self.log(\"train_loss\", losses[\"total_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"train_recon_loss\", losses[\"recon_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"train_kl_loss\", losses[\"kl_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         # Log reconstruction metrics\n",
    "#         self.log('train_precision', confusion_matrix['precision'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_recall', confusion_matrix['recall'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_f1', confusion_matrix['f1'], on_step=False, on_epoch=True)\n",
    "\n",
    "#         # Return total loss for backpropagation\n",
    "#         return losses[\"total_loss\"]\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         # Get data\n",
    "#         x = batch[0]\n",
    "\n",
    "#         # Forward pass\n",
    "#         p_x, q_z, z = self.forward(x)\n",
    "\n",
    "#         # Compute loss\n",
    "#         losses = beta_vae_loss(\n",
    "#             x=x,\n",
    "#             p_x=p_x,\n",
    "#             q_z=q_z,\n",
    "#             beta=self.beta,\n",
    "#         )\n",
    "\n",
    "#         # Compute reconstruction metrics\n",
    "#         x_recon = torch.sigmoid(p_x.logits)\n",
    "#         confusion_matrix = binary_confusion_matrix(x_recon, x, threshold=0.5)\n",
    "\n",
    "#         # Log metrics\n",
    "#         self.log(\"val_loss\", losses[\"total_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"val_recon_loss\", losses[\"recon_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"val_kl_loss\", losses[\"kl_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         # Log reconstruction metrics\n",
    "#         self.log('train_precision', confusion_matrix['precision'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_recall', confusion_matrix['recall'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_f1', confusion_matrix['f1'], on_step=False, on_epoch=True)\n",
    "        \n",
    "#         # Return total loss for backpropagation\n",
    "#         return losses[\"total_loss\"]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
