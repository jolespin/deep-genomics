{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.distributions import (\n",
    "#     Normal, \n",
    "#     Bernoulli, \n",
    "#     kl_divergence,\n",
    "# )\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.datasets import fetch_openml\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import cpu_count\n",
    "# from pathlib import Path\n",
    "# import json\n",
    "# from abc import (\n",
    "#     ABC,\n",
    "#     abstractmethod,\n",
    "# )\n",
    "# from loguru import logger\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.distributions import (\n",
    "#     Normal, \n",
    "#     Bernoulli,\n",
    "#     kl_divergence,\n",
    "# )\n",
    "# import lightning as L\n",
    "# from huggingface_hub import (\n",
    "#     HfApi, \n",
    "#     hf_hub_download,\n",
    "# )\n",
    "\n",
    "\n",
    "# from deep_genomics.utils import (\n",
    "#     get_activation_fn,\n",
    "#     ACTIVATION_MAP,\n",
    "#     set_seed,\n",
    "# )\n",
    "# from deep_genomics.losses import (\n",
    "#     beta_vae_loss, \n",
    "#     beta_tcvae_loss,\n",
    "# )\n",
    "# from deep_genomics.metrics import (\n",
    "#     binary_confusion_matrix,\n",
    "# )\n",
    "\n",
    "\n",
    "# class VariationalEncoder(nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             input_dim: int,\n",
    "#             hidden_dims: list,\n",
    "#             latent_dim: int,\n",
    "#             activation_fn = nn.ReLU,\n",
    "#         ):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dims = list(hidden_dims)\n",
    "#         self.latent_dim = latent_dim\n",
    "#         if isinstance(activation_fn, str):\n",
    "#             activation_fn = ACTIVATION_MAP[activation_fn]\n",
    "#         self.activation_fn = activation_fn\n",
    "        \n",
    "#         # Build encoder with progressive compression\n",
    "#         encoder_layers = []\n",
    "#         prev_dim = input_dim\n",
    "        \n",
    "#         for hidden_dim in hidden_dims:\n",
    "#             encoder_layers.extend([\n",
    "#                 nn.Linear(prev_dim, hidden_dim),\n",
    "#                 activation_fn(),\n",
    "#             ])\n",
    "#             prev_dim = hidden_dim\n",
    "        \n",
    "#         self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "#         # Encoder heads\n",
    "#         last_hidden_dim = hidden_dims[-1]\n",
    "#         self.fc_mu = nn.Linear(last_hidden_dim, latent_dim)\n",
    "#         self.fc_logvar = nn.Linear(last_hidden_dim, latent_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = self.encoder(x)\n",
    "#         mu = self.fc_mu(h)\n",
    "#         logvar = self.fc_logvar(h)\n",
    "#         return mu, logvar\n",
    "        \n",
    "# class VariationalDecoder(nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             input_dim: int,\n",
    "#             hidden_dims: list,\n",
    "#             latent_dim: int,\n",
    "#             activation_fn = nn.ReLU,\n",
    "#         ):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dims = list(hidden_dims)\n",
    "#         self.latent_dim = latent_dim\n",
    "#         if isinstance(activation_fn, str):\n",
    "#             activation_fn = ACTIVATION_MAP[activation_fn]\n",
    "#         self.activation_fn = activation_fn\n",
    "\n",
    "\n",
    "#         # Build decoder - mirror encoder\n",
    "#         decoder_layers = []\n",
    "#         prev_dim = latent_dim\n",
    "        \n",
    "#         for hidden_dim in reversed(hidden_dims):\n",
    "#             decoder_layers.extend([\n",
    "#                 nn.Linear(prev_dim, hidden_dim),\n",
    "#                 activation_fn()\n",
    "#             ])\n",
    "#             prev_dim = hidden_dim\n",
    "        \n",
    "#         # Final layer outputs logits (not probabilities)\n",
    "#         decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "#         self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         logits = self.decoder(z)\n",
    "#         return logits\n",
    "    \n",
    "# class BaseVAE(L.LightningModule, ABC):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             encoder: nn.Module,\n",
    "#             decoder: nn.Module,\n",
    "#             learning_rate:float,\n",
    "#             ) -> None:\n",
    "#         super().__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.learning_rate = learning_rate\n",
    "#         # Ignore encoder/decoder save the rest\n",
    "#         self.save_hyperparameters(ignore=['encoder', 'decoder'])\n",
    "#     # Default abstract methods\n",
    "#     @abstractmethod\n",
    "#     def encode(self, x: torch.Tensor):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def decode(self, z: torch.Tensor):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"Return (p_x, q_z, z) for loss computation\"\"\"\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def sample(self, n_samples:int, device=None):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def reconstruct(self, x, batch_size=2048, device=None, return_cpu=True):\n",
    "#         pass\n",
    "#     def transform(self, x, batch_size=2048, device=None, return_cpu=True):\n",
    "#         \"\"\"\n",
    "#         Transform input to latent representation (deterministic)\n",
    "        \n",
    "#         Args:\n",
    "#             x: Tensor or array of shape [n_samples, n_features]\n",
    "#             batch_size: Process in batches of this size for memory efficiency\n",
    "#             device: Device to move tensors to (e.g., 'cpu', 'mps', 'cuda'). If None, uses model's current device\n",
    "#             return_cpu: Return output on CPU\n",
    "        \n",
    "#         Returns:\n",
    "#             Latent representations of shape [n_samples, latent_dim]\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "        \n",
    "#         if device is None:\n",
    "#             device = next(self.parameters()).device\n",
    "        \n",
    "#         # Convert to tensor if needed\n",
    "#         if not isinstance(x, torch.Tensor):\n",
    "#             x = torch.from_numpy(x).float()\n",
    "        \n",
    "#         n_samples = x.shape[0]\n",
    "#         latent_codes = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             # Process in batches for memory efficiency\n",
    "#             for i in range(0, n_samples, batch_size):\n",
    "#                 batch = x[i:i+batch_size].to(device)\n",
    "#                 mu, _ = self.encode(batch)\n",
    "#                 latent_codes.append(mu.cpu() if return_cpu else mu)\n",
    "        \n",
    "#         return torch.cat(latent_codes, dim=0)\n",
    "\n",
    "#     # Lightning Methods\n",
    "#     @abstractmethod\n",
    "#     def configure_optimizers(self):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         pass\n",
    "#     @abstractmethod\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         pass\n",
    "#     # @abstractmethod\n",
    "#     # # def test_step(self, batch, batch_idx):\n",
    "#     #     pass\n",
    "#     # @abstractmethod\n",
    "#     # # def predict_step(self, batch, batch_idx):\n",
    "#     #     pass\n",
    "\n",
    "#     # HuggingFace\n",
    "#     def save_pretrained(self, save_directory):\n",
    "#         \"\"\"\n",
    "#         Save model weights and config in HuggingFace format.\n",
    "        \n",
    "#         Args:\n",
    "#             save_directory: Path to directory where model will be saved\n",
    "#         \"\"\"\n",
    "#         save_directory = Path(save_directory)\n",
    "#         save_directory.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         # Save model weights (PyTorch state_dict)\n",
    "#         weights_path = save_directory / \"pytorch_model.bin\"\n",
    "#         torch.save(self.state_dict(), weights_path)\n",
    "        \n",
    "#         # Save hyperparameters as config\n",
    "#         config_path = save_directory / \"config.json\"\n",
    "#         with open(config_path, 'w') as f:\n",
    "#             json.dump(self.hparams, f, indent=2, default=str)\n",
    "        \n",
    "#         print(f\"Model saved to {save_directory}\")\n",
    "    \n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, pretrained_model_path, map_location=None):\n",
    "#         \"\"\"\n",
    "#         Load model from HuggingFace format or local directory.\n",
    "        \n",
    "#         Args:\n",
    "#             pretrained_model_path: Local path or HuggingFace Hub model ID\n",
    "#             map_location: Device to load model weights (e.g., 'cpu', 'cuda', 'mps')\n",
    "        \n",
    "#         Returns:\n",
    "#             Loaded model instance\n",
    "#         \"\"\"\n",
    "#         pretrained_model_path = Path(pretrained_model_path)\n",
    "        \n",
    "#         # Load config\n",
    "#         config_path = pretrained_model_path / 'config.json'\n",
    "#         if not config_path.exists():\n",
    "#             raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "        \n",
    "#         with open(config_path, 'r') as f:\n",
    "#             config = json.load(f)\n",
    "        \n",
    "#         # Create model instance (subclass must handle this)\n",
    "#         # This is why it's a classmethod - subclass implements construction logic\n",
    "#         model = cls(**config)\n",
    "        \n",
    "#         # Load weights\n",
    "#         weights_path = pretrained_model_path / 'pytorch_model.bin'\n",
    "#         if not weights_path.exists():\n",
    "#             raise FileNotFoundError(f\"Weights file not found: {weights_path}\")\n",
    "        \n",
    "#         state_dict = torch.load(weights_path, map_location=map_location)\n",
    "#         model.load_state_dict(state_dict)\n",
    "        \n",
    "#         print(f\"Model loaded from {pretrained_model_path}\")\n",
    "#         return model\n",
    "    \n",
    "#     def push_to_hub(\n",
    "#         self,\n",
    "#         repo_id: str,\n",
    "#         commit_message: str = \"Upload model\",\n",
    "#         private: bool = False,\n",
    "#         token: str = None,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Upload model to HuggingFace Hub.\n",
    "        \n",
    "#         Args:\n",
    "#             repo_id: Repository ID on HuggingFace Hub (e.g., \"username/model-name\")\n",
    "#             commit_message: Commit message for the upload\n",
    "#             private: Whether to make the repository private\n",
    "#             token: HuggingFace API token (or set HF_TOKEN environment variable)\n",
    "        \n",
    "#         Example:\n",
    "#             model.push_to_hub(\"myusername/binary-vae-mnist\")\n",
    "#         \"\"\"\n",
    "#         from huggingface_hub import HfApi\n",
    "        \n",
    "#         # Save to temporary directory\n",
    "#         import tempfile\n",
    "#         with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "#             self.save_pretrained(tmp_dir)\n",
    "            \n",
    "#             # Upload to Hub\n",
    "#             api = HfApi()\n",
    "#             api.create_repo(\n",
    "#                 repo_id=repo_id,\n",
    "#                 private=private,\n",
    "#                 exist_ok=True,\n",
    "#                 token=token,\n",
    "#             )\n",
    "            \n",
    "#             api.upload_folder(\n",
    "#                 folder_path=tmp_dir,\n",
    "#                 repo_id=repo_id,\n",
    "#                 commit_message=commit_message,\n",
    "#                 token=token,\n",
    "#             )\n",
    "        \n",
    "#         print(f\"Model uploaded to https://huggingface.co/{repo_id}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9b75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BinaryBetaVAE(BaseVAE):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             # Architecture\n",
    "#             input_dim: int,\n",
    "#             hidden_dims: list,\n",
    "#             latent_dim: int,\n",
    "#             activation_fn = \"ReLU\",\n",
    "#             # Optimizer\n",
    "#             learning_rate:float = 1e-3,\n",
    "#             # Loss\n",
    "#             beta:float = 1.0,\n",
    "#             # Sub-models\n",
    "#             encoder: nn.Module = None,\n",
    "#             decoder: nn.Module = None,\n",
    "#             ) -> None:\n",
    "#         # Get activation function\n",
    "#         if not isinstance(activation_fn, str):\n",
    "#             activation_fn = activation_fn.__name__\n",
    "#         # Build encoder/decoder if not provided\n",
    "#         if all([\n",
    "#             encoder is None,\n",
    "#             decoder is None,\n",
    "#             ]):\n",
    "#             # Neither provided - build both\n",
    "#             if any([\n",
    "#                 input_dim is None, \n",
    "#                 hidden_dims is None, \n",
    "#                 latent_dim is None,\n",
    "#                 ]):\n",
    "#                 raise ValueError(\n",
    "#                     \"Must provide either (encoder, decoder) or \"\n",
    "#                     \"(input_dim, hidden_dims, latent_dim)\"\n",
    "#                 )\n",
    "#             encoder = VariationalEncoder(input_dim, hidden_dims, latent_dim, activation_fn)\n",
    "#             decoder = VariationalDecoder(input_dim, hidden_dims, latent_dim, activation_fn)\n",
    "            \n",
    "#         elif encoder is None or decoder is None:\n",
    "#             # Only one provided - error\n",
    "#             raise ValueError(\"Must provide both encoder and decoder, or neither\")\n",
    "            \n",
    "#         else:\n",
    "#             # Both provided - validate they match\n",
    "#             if encoder.input_dim != decoder.input_dim:\n",
    "#                 raise ValueError(\"Encoder and decoder input_dim must match\")\n",
    "#             if list(encoder.hidden_dims) != list(decoder.hidden_dims):\n",
    "#                 raise ValueError(\"Encoder and decoder hidden_dims must match\")\n",
    "#             if encoder.latent_dim != decoder.latent_dim:\n",
    "#                 raise ValueError(\"Encoder and decoder latent_dim must match\")\n",
    "            \n",
    "#             # Infer architecture from encoder\n",
    "#             input_dim = encoder.input_dim\n",
    "#             hidden_dims = encoder.hidden_dims\n",
    "#             latent_dim = encoder.latent_dim\n",
    "#         super().__init__(encoder, decoder, learning_rate)\n",
    "#         # Store archtecture metadata\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dims = list(hidden_dims)\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.activation_fn = activation_fn\n",
    "#         self.beta = beta\n",
    "#         self.save_hyperparameters(ignore=['encoder', 'decoder'])\n",
    "\n",
    "#     def encode(self, x: torch.Tensor):\n",
    "#         return self.encoder(x)\n",
    "#     def decode(self, z: torch.Tensor):\n",
    "#         return self.decoder(z)\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"Return (p_x, q_z, z) for loss computation\"\"\"\n",
    "#                 # Encode\n",
    "#         mu, logvar = self.encode(x)\n",
    "\n",
    "#         # Reparameterization\n",
    "#         std = torch.exp(0.5 * logvar)\n",
    "#         # q_z is the approximate posterior - q(z|x)\n",
    "#         q_z = Normal(mu, std)\n",
    "#         z = q_z.rsample()\n",
    "\n",
    "#         # Decode\n",
    "#         logits = self.decode(z)\n",
    "#         # p_x is the likelihood - p(x|z)\n",
    "#         p_x = Bernoulli(logits=logits)\n",
    "\n",
    "#         return p_x, q_z, z\n",
    "    \n",
    "#     def sample(self, n_samples:int, device=None):\n",
    "#         \"\"\"\n",
    "#         Generate samples from the prior p(z) = N(0, I)\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "#         if device is None:\n",
    "#             device = next(self.parameters()).device\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             # Sample from distribution\n",
    "#             z = torch.randn(n_samples, self.latent_dim, device=device)\n",
    "        \n",
    "#             # Decode to get reconstructions\n",
    "#             logits = self.decode(z)\n",
    "#             samples = torch.sigmoid(logits)\n",
    "            \n",
    "#         return samples\n",
    "#     def reconstruct(self, x, batch_size=2048, device=None, return_cpu=True):\n",
    "#         \"\"\"\n",
    "#         Reconstruct input using posterior mean (deterministic)\n",
    "\n",
    "#         Args:\n",
    "#             x: Tensor or array of shape [n_samples, n_features] or [n_features]\n",
    "#             batch_size: Process in batches of this size for memory efficiency\n",
    "#             device: Device to move tensors to (e.g., 'cpu', 'mps', 'cuda'). If None, uses model's current device\n",
    "#             return_cpu: Return output on CPU\n",
    "        \n",
    "#         Returns:\n",
    "#             Reconstruction(s) of shape [n_samples, n_features] or [n_features]\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "        \n",
    "#         if device is None:\n",
    "#             device = next(self.parameters()).device\n",
    "        \n",
    "#         # Handle single sample\n",
    "#         squeeze_output = False\n",
    "#         if isinstance(x, torch.Tensor) and x.ndim == 1:\n",
    "#             x = x.unsqueeze(0)\n",
    "#             squeeze_output = True\n",
    "#         elif isinstance(x, np.ndarray) and x.ndim == 1:\n",
    "#             x = x.reshape(1, -1)\n",
    "#             squeeze_output = True\n",
    "        \n",
    "#         # Convert to tensor if needed\n",
    "#         if not isinstance(x, torch.Tensor):\n",
    "#             x = torch.from_numpy(x).float()\n",
    "        \n",
    "#         n_samples = x.shape[0]\n",
    "#         reconstructions = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             # Process in batches for memory efficiency\n",
    "#             for i in range(0, n_samples, batch_size):\n",
    "#                 batch = x[i:i+batch_size].to(device)\n",
    "#                 mu, _ = self.encode(batch)\n",
    "#                 logits = self.decode(mu)\n",
    "#                 x_recon = torch.sigmoid(logits)\n",
    "#                 reconstructions.append(x_recon.cpu() if return_cpu else x_recon)\n",
    "        \n",
    "#         result = torch.cat(reconstructions, dim=0)\n",
    "        \n",
    "#         # Remove batch dimension if input was single sample\n",
    "#         if squeeze_output:\n",
    "#             result = result.squeeze(0)\n",
    "        \n",
    "#         return result\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             self.parameters(), \n",
    "#             lr=self.learning_rate,\n",
    "#         )\n",
    "#         return optimizer\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         # Get data\n",
    "#         x = batch[0]\n",
    "\n",
    "#         # Forward pass\n",
    "#         p_x, q_z, z = self.forward(x)\n",
    "\n",
    "#         # Compute loss\n",
    "#         losses = beta_vae_loss(\n",
    "#             x=x,\n",
    "#             p_x=p_x,\n",
    "#             q_z=q_z,\n",
    "#             beta=self.beta,\n",
    "#         )\n",
    "\n",
    "#         # Compute reconstruction metrics\n",
    "#         x_recon = torch.sigmoid(p_x.logits)\n",
    "#         confusion_matrix = binary_confusion_matrix(x_recon, x, threshold=0.5)\n",
    "\n",
    "#         # Log loss\n",
    "#         self.log(\"train_loss\", losses[\"total_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"train_recon_loss\", losses[\"recon_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"train_kl_loss\", losses[\"kl_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         # Log reconstruction metrics\n",
    "#         self.log('train_precision', confusion_matrix['precision'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_recall', confusion_matrix['recall'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_f1', confusion_matrix['f1'], on_step=False, on_epoch=True)\n",
    "\n",
    "#         # Return total loss for backpropagation\n",
    "#         return losses[\"total_loss\"]\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         # Get data\n",
    "#         x = batch[0]\n",
    "\n",
    "#         # Forward pass\n",
    "#         p_x, q_z, z = self.forward(x)\n",
    "\n",
    "#         # Compute loss\n",
    "#         losses = beta_vae_loss(\n",
    "#             x=x,\n",
    "#             p_x=p_x,\n",
    "#             q_z=q_z,\n",
    "#             beta=self.beta,\n",
    "#         )\n",
    "\n",
    "#         # Compute reconstruction metrics\n",
    "#         x_recon = torch.sigmoid(p_x.logits)\n",
    "#         confusion_matrix = binary_confusion_matrix(x_recon, x, threshold=0.5)\n",
    "\n",
    "#         # Log metrics\n",
    "#         self.log(\"val_loss\", losses[\"total_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"val_recon_loss\", losses[\"recon_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log(\"val_kl_loss\", losses[\"kl_loss\"], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         # Log reconstruction metrics\n",
    "#         self.log('train_precision', confusion_matrix['precision'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_recall', confusion_matrix['recall'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "#         self.log('train_f1', confusion_matrix['f1'], on_step=False, on_epoch=True)\n",
    "        \n",
    "#         # Return total loss for backpropagation\n",
    "#         return losses[\"total_loss\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b79aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST...\n",
      "Train samples: 48000\n",
      "Validation samples: 12000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST\n",
    "print(\"Loading MNIST...\")\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='auto')\n",
    "X = (X > 0).astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "# Split into train/test (MNIST default split)\n",
    "X_train_full, X_test = X[:60000], X[60000:]\n",
    "y_train_full, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# Split train into train/validation (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, \n",
    "    y_train_full, \n",
    "    test_size=0.2,  # 20% for validation\n",
    "    random_state=42,\n",
    "    stratify=y_train_full  # Maintain class distribution\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train),\n",
    "    torch.from_numpy(y_train),\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_val),\n",
    "    torch.from_numpy(y_val),\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_test),\n",
    "    torch.from_numpy(y_test),\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "num_workers = cpu_count() - 1\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=False,  # Don't shuffle validation\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4450fd60",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deep_genomics.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Training function\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mL\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeep_genomics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BinaryBetaVAE\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeep_genomics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_seed\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloggers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorBoardLogger\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'deep_genomics.models'"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "import lightning as L\n",
    "from deep_genomics.models.vae import BinaryBetaVAE\n",
    "from deep_genomics.utils import set_seed\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "# from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "\n",
    "\n",
    "device = \"mps\"\n",
    "set_seed(42)\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"vae_testing\")\n",
    "\n",
    "input_dim = train_dataset[0][0].shape[0]\n",
    "hidden_dims = [512,256,128]\n",
    "latent_dim = 24\n",
    "model = BinaryBetaVAE(\n",
    "    input_dim=input_dim, \n",
    "    hidden_dims=hidden_dims, \n",
    "    latent_dim=latent_dim,\n",
    ")\n",
    "limit_train_batches=None\n",
    "max_epochs=10\n",
    "trainer = L.Trainer(\n",
    "    logger=logger, \n",
    "    limit_train_batches=limit_train_batches, \n",
    "    max_epochs=max_epochs, \n",
    "    accelerator=device,\n",
    ")\n",
    "trainer.fit( model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4eafb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = X_test[0]\n",
    "x_recon = model.reconstruct(x) #> 0.5\n",
    "fig, axes = plt.subplots(1, 2, figsize=(5,5))\n",
    "\n",
    "axes[0].imshow(x.reshape(28,28), cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(x_recon.reshape(28,28), cmap='gray')\n",
    "axes[1].set_title('Reconstruction')\n",
    "axes[1].axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd535b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to lightning_logs/vae_testing/pretrained_model\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"lightning_logs/vae_testing/pretrained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from lightning_logs/vae_testing/pretrained_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model2 = BinaryBetaVAE.from_pretrained(\"lightning_logs/vae_testing/pretrained_model\", map_location=\"mps\")\n",
    "x = X_test[0]\n",
    "x_recon = model2.reconstruct(x) #> 0.5\n",
    "fig, axes = plt.subplots(1, 2, figsize=(5,5))\n",
    "\n",
    "axes[0].imshow(x.reshape(28,28), cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(x_recon.reshape(28,28), cmap='gray')\n",
    "axes[1].set_title('Reconstruction')\n",
    "axes[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f62553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /var/folders/kz/sjn7w_851dld8bdf3jf6hbw80000gn/T/tmp2962v7k3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42235ee8c4f74e2bb95fd589de9907d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2eafab94984286b196b7abd03820bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to https://huggingface.co/jolespin/binary-vae-mnist\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"\"\n",
    "model.push_to_hub(\"jolespin/binary-vae-mnist\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89c58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_latent = model.transform(torch.from_numpy(X_train).to(device)).cpu().numpy()\n",
    "\n",
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Convert tensor to numpy array\n",
    "\n",
    "# # Run UMAP to reduce to 2D\n",
    "# reducer = umap.UMAP(n_components=2)\n",
    "# X_umap = reducer.fit_transform(X_latent)\n",
    "\n",
    "# # Ensure labels are integers for coloring\n",
    "# labels = y_train.astype(int)\n",
    "\n",
    "# # Plot with digit colors\n",
    "# plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, cmap='tab10', s=5)\n",
    "# plt.title('UMAP projection (MNIST digits colored)')\n",
    "# plt.xlabel('UMAP 1')\n",
    "# plt.ylabel('UMAP 2')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8730d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
